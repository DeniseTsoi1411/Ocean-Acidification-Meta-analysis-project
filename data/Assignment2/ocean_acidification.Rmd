---
title: "Ocean Acidification publication bias project"
output: html_document
date: "2022-10-17"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## **(1) Statistical Analysis and Interpretation (50%)**
### 1. Analysis of Clark et al. (2020) data (i.e., OA_activitydat_20190302_BIOL3207.csv)
```{r}
library(readr)
library(tidyverse)
OA_data <- read_csv("./OA_activitydat_20190302_BIOL3207.csv")
clark_data <- read_csv("./clark_paper_data.csv")
OA_summary <- OA_data %>% group_by(species, treatment) %>%
              summarise(mean = mean(activity, na.rm = TRUE),
                        sd = sd(activity, na.rm = TRUE),
                        n = length(unique(animal_id))) %>%
              rename(Species = "species")
```
Preliminary analysis was done by producing a summary statistic for the whole dataset, where the statistics of Mean, Standard deviation and sample size is given for each fish species' average activity after each treatment.

### 2. Merge the summary statistics from above with the metadata provided as 'clark_paper_data.csv' from Clark et al. (2020).
```{r}
OA_clark <- cbind(clark_data, OA_summary)
OA_clark1 <- pivot_wider(OA_clark, names_from = treatment,
                     names_glue = "{treatment}_{.value}",
                     values_from = c("mean", "sd", "n"))
```
After binding the two datasets together, it was then transformed into a wider format. This was done because the focus was on the changes of fish behaviour depemding on the treatment. Thus, for each treatment, we looked at their respective mean, standard deviation and sample size.

### 3. Merging the combined dataset with the larger meta-analysis dataset (i.e., ocean_meta_data.csv).
```{r}
## 3a) Reading the meta-analysis dataset
ocean_meta <- read_csv("./ocean_meta_data.csv")

## 3b) Rename the columns such that it matches with ones from 'ocean_meta'
OA_clark2 <- OA_clark1 %>% rename("oa.mean" = CO2_mean,
                            "oa.sd" = CO2_sd,
                            "oa.n" = CO2_n,
                            "ctrl.mean" = control_mean,
                            "ctrl.sd" = control_sd,
                            "ctrl.n" = control_n)

## 3c) Reorder the column names based on the ones in 'ocean_meta'
OA_clark2 <- OA_clark2[names(ocean_meta)]

## 3c2) Double check sequence of column names
colnames(ocean_meta) == colnames(OA_clark2) # Should return all TRUE

## 3d) Bind the two dataframes
final <- rbind(ocean_meta, OA_clark2)
```

### 4. Calculate the log response ratio (lnRR) effect size for every row of the dataframe using metaforâ€™s escalc() function.
```{r}
## 4a) Load the packages
library(orchaRd)
library(metafor)

## 4b) Calculating the ROM values from the 'final' data
rom <- metafor::escalc(measure = "ROM", 
                        m1i = ctrl.mean,
                        m2i = oa.mean,
                        sd1i = ctrl.sd,
                        sd2i = oa.sd,
                        n1i = ctrl.n,
                        n2i = oa.n,
                        data = final,
                        var.names = c("ROM", "ROM_V"))
rom <- mutate(rom, residual = 1:n()) # Added a column in 'rom' for observations / residuals 

plot(rom$residual, rom$ROM_V) # Observed outliers in ROM_V, deleted them for further analysis
which(rom$ROM_V > 300) # To find unusually large ROM_V values
which(rom$ROM_V < 0.0001) # To find unusually small ROM_V values
rom1 <- rom[-c(163, 220, 673, 6, 16, 17, 255, 497, 562, 564),] # Made new variable with deleted outliers
```
Background notes:
- orcharRd package = A package that helps with the creation of orchard plots, containing both confidence and prediction intervals around point estimates for different categorical moderators. It also plots effect size distribution with it weighed with either precision (1/Standard error, SE) or sample size.  

- metafor package = A comprehensive package used for meta-analysis. This package also helps with the calculation of effect size. When used in conjuction with orchaRd, it can plot a meta-analytic or meta-regression model. 

- escalc function = Helps with the measure of various effect sizes or outcome measures. The 'measure = "ROM"' and 'ROM_V' was used to observe the log transformed ratio of the means, such that the changes in behavioural patterns are standardised and made easier to compare.

- LnRR = The comparison of the mean effects from the treatment and mean effects from the control group.

- Effect size = Measures the strength of a relationship between two variables. A low value indicates a small significance between their relationship, while a larger value indicates otherwise.

Upon inspecting the values within "ROM_V", it was clear that there are many extreme outliers. The residuals with extremely high "ROM_V" values are: 163, 220 and 673 (Over 300). It is possible that the ratio between the mean and standard deviation are disproportionate compared to the other observations, resulting in larger values. 

Similarly, observations 6, 16, 17, 255, 497, 562 and 564 have "ROM_V" values less than e-04. While assessing the relevant columns, both "ctrl.sd" and "oa.sd" values ranges from 0.0001 to over 30,000. This is not characteristic of standard deviation because a distance of range 0.0001 to 30,000 units from the mean is highly unlikely. Thus, the standard deviation column is likely to be mislabeled, and should be represented as standard error instead. 

The outliers are deleted for subsequent analysis.

Some "ROM" and "ROM_V" values are "NA". This occurs when there is negative "oa.mean" values, but positive "ctrl.mean" values. However, they were not deleted from subsequent analysis.


### 5. Designing a Meta-analytic model using the log transformed ratios ('ROM' and 'ROM_V')
```{r}
MLMA <- metafor:: rma.mv(ROM ~ 1 , V = ROM_V,
                         method = "REML",
                         random=list(~1 | Study,
                                     ~1 | Species,
                                     ~1 | residual),
                         test = "t", dfs = "contain", 
                         data = rom1)
```
Background:
- MLE = A systematic method for parameter estimations, but its variance estimator tends to be biased (It over- or under-estimates the true variance).
- REML = Restricted Maximum Likelihood = A method for fitting linear mixed models. It can produce unbiased estimates of variance and covariance parameters

The 'MLMA' model is derived by using the REML method to fit the log transformed ratios against its variances. It takes into account the random effects of study and observations, and are fitted to control the sampling variance of lnRR.

### 6. Further analysis with plots and statistics
```{r}
summary(MLMA) # To get intercept and 95% confidence intervals
pi_val <- predict(MLMA) 
pi_val # To get 95% Prediction interval
I2_val <-  orchaRd::i2_ml(MLMA) 
I2_val # Shows dissection of variation in effects within and across studies

# Made a new multilevel meta-regression model to observe ROM against moderator "Climate..FishBase."
MLMR <-  metafor:: rma.mv(ROM ~ Climate..FishBase., V = ROM_V,
                          method = "REML",
                          random=list(~1 | Study,
                                      ~1 | Species,
                                      ~1 | residual),
                          test = "t", dfs = "contain", 
                          data = rom1)
```
An intercept in this context reflects the overall estimate of the effect from all experiments published in the literature. In other words, it combines the meta-analytic mean effect sizes across all studies. With an intercept of -0.0629, it infers a weak negative correlation between elevation of CO2 levels and fish behavior across all studies. Therefore, an increase in CO2 levels can reflect a weak relationship with a decrease in changes of fish behaviour.

The resultant 95% confidence and prediction intervals are -0.3127 to 0.1868 and -4.2762 to 4.1503, respectively. While 95% of the time we can be confident the true mean would fall between LnRR values of -0.3127 to 0.1868, we also expect effect sizes to range from -4.2762 to 4.1503 with 95% of repeated experiments. Such a broad prediction interval is indicative of high inconsistency between studies. 

```{r orchard1, fig.align='center', fig.cap="Figure 1: Orchard plot with Log repsonse ratio (lnRR) as effect size for Ocean acidification and fish behaviour in relations to Climate conditions. k = Effect size, Number of studies in brackets, scaled to precision of each effect size value (1/ROM_V)" }

orchaRd::orchard_plot(MLMR, mod = "Climate..FishBase.", group = "Study", data = rom, xlab = "Log response ratio effect size (lnRR)", angle = 45) +
  annotate(geom = "text", x = 1, y = -10, 
           label = paste0("italic(I)^{2} == ",round(I2_val[1], 2)), 
           color = "black", parse = TRUE, size = 5) + 
  annotate(geom = "text", x = 0.95, y = -7, label = "%", color = "black", parse = FALSE, size = 5) 
```
Aside from prediction intervals, we can also deduce levels of heterogeneity through I^2. I^2 represents the allocation of variation from within and across studies into "Total" and the aforementioned random effects. Evidently, we observe complete heterogeneous effect size because sampling variance contributes 0.1% (100% - I(total)^2) of total variation in effect size. Likewise, we observe 5.69% of total effect size variation due to differences in studies, 5.08% due to differences in species, and 89.11% due to residual effects. 

The MLMR model and Orchard plot relates the effect sizes with the climate, evaluating whether lnRR value changes depending on the climate conditions. Thus, the intercept of MLMR explains the relationship between CO2 levels and fish behavior in correspondence with climate. As indicated on the Orchard plot as well, we observe majority of points with very low precision, with 95% confidence interval intersecting x=0 (Thick black line), suggesting no effect. The 95% prediction interval (Thin black line) reflects the high heterogeneity (99.89%), with "k" showing the effect size and values in bracket shows sample size for that climate group.

### 7. Funnel plot for visually assessing the possibility of publication bias.
```{r funnel1, fig.align='center', fig.cap= "Figure 2: Funnel plot showing correlation between Ocean acidification and fish behaviour depending on the precision (1/SE). Dotted lines show theoretical 95% sampling variance intervals (Where we would expect effect size values to fall). Pink and grey shaded regions shows significance of p-values: Grey shows non-significance, while pinks approaching lighter shades have p-values of 0.01 to 0.05, 0.05 to 0.1, and 0.1 to 1.0, respectively."}
metafor::funnel(x = rom1$ROM, vi = rom1$ROM_V, 
                yaxis = "seinv", digits = 2, 
                level = c(0.1, 0.05, 0.01), 
                shade = c("lightpink", "lightcoral", "indianred2"), las = 1, 
                xlab = "Log response ratio (lnRR)", legend = TRUE)
```

### 8. Time-lag plot assessing how effect sizes may or may not have changed through time
```{r timelag1, fig.align='center', fig.cap= "Figure 3: Time lag plot to assess Log response ratio against the year it became online. Point sizes are scaled depending on precision (1/ROM_V), with smaller points indicating lower precision and high sample variance."}
ggplot(rom1, aes(y = ROM, x = Year..online., size = 1/ROM_V)) + geom_point(alpha = 0.3) +
    geom_smooth(method = lm, col = "red", show.legend = FALSE) + labs(x = "Year online",
    y = "Log response ratio (lnRR)", size = "Precision (1/SE)") +
    theme_classic()
```
Generally, effect size explains how meaningful a relationship is between variables. In this case, where there are increasingly more publications about Ocean acidification, we can assess the changes in effect size over the years, and the extent in which these values are trustworthy.

### 9. Formal meta-regression model that includes year as a moderator (fixed effect) to test for time-lag bias
```{r}
rom1 <- rom1 %>% mutate(Year_c = Year..online. - mean(Year..online.)) 
MLMR1 <-  metafor:: rma.mv(ROM ~ Year_c, V = ROM_V,
                          method = "REML",
                          random=list(~1 | Study,
                                      ~1 | Species,
                                      ~1 | residual),
                          test = "t", dfs = "contain", 
                          data = rom1)
summary(MLMR1)
r2_1 <- orchaRd::r2_ml(MLMR1)
r2_1
```

### 10. Formal meta-regression model that includes inverse sampling var (i.e., 1vlnRR) to test for file-drawer biases
```{r}
rom1 <- rom1 %>% mutate(inv_lnRR = 1/(ROM_V))
MLMR2 <-  metafor:: rma.mv(ROM ~ Year_c + inv_lnRR, V = ROM_V,
                          method = "REML",
                          random=list(~1 | Study,
                                      ~1 | Species,
                                      ~1 | residual),
                          test = "t", dfs = "contain", 
                          data = rom1)
summary(MLMR2)
r2_2 <- orchaRd::r2_ml(MLMR2)
r2_2
```

### 11. Analysing presence of publication bias based on meta-regression results (Type of publication bias, how does it change the analysis, what may be contributing to the bias)

Overall, there is limited presence of publication bias shown in the meta-regression results. 

Firstly, Funnel plots helps to expose presence of publication bias within the dataset. As seen in figure 2, there are more studies with lower precision (1/SE) on the negative end of the lnRR spectrum, while the number of higher lnRR values increase as precision increases. This display is indicative of an absence of file-drawer bias, as we would expect studies with low precision to be 'stashed away', hence would observe less studies with negative lnRR with low precision. 

However, the pink and grey areas in the graph indicates areas of significance, where sampling variance does affect the effect size. With no pink area visible, it is deduced that none of the studies shows findings with significant p-values, therefore, no significant correlation between Ocean acidification and Fish behaviour.

Secondly, the time-lag plot shows the evolution of studies' precision and effect size over time. In other words, studies with negative effects tends to take longer to publish than positive ones. Figure 3 shows a weak positive relationship between lnRR and time through the red LOWESS curve, thus weak presence of time-lag bias. Early on, there were many studies with small precision and lnRR values. This could indicate many low quality data due to large sample error or small sample size. As time passes, more low precision studies have moderately increased lnRR values. There is also an increased occurrence of higher precision studies. With less studies converging away from the red line as time passes, we can assume that recent studies have been more closely able to describe the relationship with Ocean acidification and fish behaviour.

Similarly, MLMR1 model has the centered year online as a moderator, which categorises the variation of effect size depending on the year. In this case, "R2_marginal" is 0.022, indicating that the time-lag explains 2.21% of the variation in lnRR among studies over time. The lack of time lag is observed in figure 3 as well, with studies with low lnRR decreases over time. 

Lastly, MLMR2 conceptualises the presence of both time-lag and file-drawer bias due to moderators being both year online and inverse of lnRR variance. The "R2_marginal" for MLMR2 is 0.027, indicating that both file-drawer and time-lag accounts for 2.74% of variation in lnRR.  

Note that the intercept for MLMR1 and MLMR2 is -0.1131 and -0.1648, respectively. MLMR1 intercept shows that the mean effect size is -0.1131 during the average year. MLMR2 has mean effect size of -0.1648 during average year and when inversed lnRR variance is 0. With the difference of both models dependent on the presence of inverse lnRR variance, we can assume that the differences in the intercepts accounts for the the amount of variation explained by the inverse effect size variance, which is -0.0517. In other words, when the inversed lnRR variance is 0 (and therefore just variance as well), the mean effect size is -0.0517. 

### 12. Identify any studies contributing to publication bias. How do your updated meta-analysis results compare with a meta-analysis by Clement et. al. (2022)? Are there any concerns about these studies? If so, describe using references to existing papers what concerns have been raised?

Clement et al., explored the presence of decline effect (decreased effect size over time) through dissection of multiple biases that may be prevalent in Ocean acidification and behavioural studies. Particularly, they assessed evidence for publication bias (Strong effect studies tends to be more readily published in higher-impact journals), citation bias (Those in higher-impact journals tends to be cited more, thus have stronger influence), methodological bias (Chosen experimental design or sample size inflates the effect size) and investigator effect (subsequent papers from same author decreases effect size).

Figure 1 of their report noted the presence of decline effect. Both a and b explained the relationship between lnRR magnitude in accordance to year online. The 95% confident LOWESS curve in figure 1a exhibited higher lnRR magnitude during the first year, and had a sharp decline until 2012. Similarly, 2019 studies in figure 1b exhibited the largest 95% confidence bounds, displaying a large uncertainty. Thus, they confirmed the presence of decline effect. 

Although they hypothesized that the presence of decline effect stemmed from the aforementioned biases and effects, they still investigated other factors that could inflate effect size. Such factors include mechanistic differences of warm and cold water fish species, isolation of olfactory cues, and sampling of only larvae. While they found no significant effect in the factors, they noted discrepancies in the methodology due to no actual background CO2 levels reported. However, the lack of signal indicated that the decline effect could be present due to the biases. 

Publication and citation bias can be categorised into one group, as publication bias tends to fuel citation bias. When assessing the proportion of effect size magnitude with journal impact factor and Google Scholar citations, they noted that the first 3 studies has much higher citation rates than others, including one from 2012. This leads to unparallel influences from those studies, thus misleads the general public. Those papers are: Munday et al (2009), Dixson et al (2009), Munday et al (2010), and Nilsson et al (2012).

They also assessed the presence of Methodological bias, which was done by correlating the mean sample size of studies with the mean effect size magnitude. They concluded that studies with high lnRR tends to have low sample size (less than 3 fishes). This can have confounding effects as low sample size correlates to lower quality since it becomes more prone to statistical errors such as Type 1 and Type 2 errors. In the supplementary 1 information, these studies are: a2, a3 and a31, and they had sample sizes of 45, 18 and 11, respectively. In supplementary 5, these studies are Dixon et al (2009), Munday et al (2010), and Munday et al (2014). 

The analysis of investigator effect involved removing early studies with similar lead investigators. In total, they removed 45% of those studies, most of which were studies before 2012, and revealed that the decline effect was no longer visible. Thus, they concluded the contribution of investigator effect onto decline effect. 

The discrepancy between the updated meta-analyses and the findings from Clement et al can relate to the deletion of outliers early on. Due to the extremities of some lnRR variance, it involved the deletion of 10 observations, including Munday et al (2009), Dixson et al (2009), and Munday et al (2012). This already lowers the effects of investigator, methodological, and both publication and citation bias. As such, we are left with low levels of biases, with time-lag explaining 2.21% of variation in lnRR across studies, and both file-drawer and time-lag explaining 2.74%. 

